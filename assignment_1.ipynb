{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "injured-calculation",
   "metadata": {},
   "source": [
    "# Ex. 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-intake",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\theta} = \\left(\\tilde X^T\\cdot \\tilde X\\right)^{-1}\\cdot\\tilde X^T\\cdot Y\\text{.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complete-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class LinearRegr:\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # input:\n",
    "        #  X = np.array, shape = (n, m)\n",
    "        #  Y = np.array, shape = (n)\n",
    "        # Finds theta minimising quadratic loss function L, using an explicit formula.\n",
    "        # Note: before applying the formula to X one should append to X a column with ones.\n",
    "        n, m = X.shape\n",
    "        X = np.c_[np.ones(n), X] #r_ would stack on the bottom\n",
    "        temp_mat = X.T @ X\n",
    "        if np.linalg.det(temp_mat) == 0:\n",
    "            raise ValueError('Not able to inverse matrix')\n",
    "        self.theta = (np.linalg.inv(temp_mat) @ X.T) @ Y #det cannot be 0\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # input:\n",
    "        #  X = np.array, shape = (k, m)\n",
    "        # returns:\n",
    "        #  Y = wektor(f(X_1), ..., f(X_k))\n",
    "        k, m = X.shape\n",
    "        X = np.c_[np.ones(k), X]\n",
    "        Y = X @ self.theta\n",
    "        return Y\n",
    "\n",
    "\n",
    "def test_RegressionInOneDim():\n",
    "    X = np.array([1,3,2,5]).reshape((4,1))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    a = np.array([1,2,10]).reshape((3,1))\n",
    "    expected = LinearRegression().fit(X, Y).predict(a)\n",
    "    actual = LinearRegr().fit(X, Y).predict(a)\n",
    "    print(f'actual: {actual}')\n",
    "    print(f'expected: {expected}')\n",
    "    assert list(actual) == pytest.approx(list(expected))\n",
    "\n",
    "def test_RegressionInThreeDim():\n",
    "    X = np.array([1,2,3,5,4,5,4,3,3,3,2,5]).reshape((4,3))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    a = np.array([1,0,0, 0,1,0, 0,0,1, 2,5,7, -2,0,3]).reshape((5,3))\n",
    "    expected = LinearRegression().fit(X, Y).predict(a)\n",
    "    actual = LinearRegr().fit(X, Y).predict(a)\n",
    "    print(f'actual: {actual}')\n",
    "    print(f'expected: {expected}')\n",
    "    assert list(actual) == pytest.approx(list(expected))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "overhead-fluid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: [ 1.8         3.34285714 15.68571429]\n",
      "expected: [ 1.8         3.34285714 15.68571429]\n"
     ]
    }
   ],
   "source": [
    "test_RegressionInOneDim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "short-corner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: [ 2.25 -1.75  2.75  2.    3.75]\n",
      "expected: [ 2.25 -1.75  2.75  2.    3.75]\n"
     ]
    }
   ],
   "source": [
    "test_RegressionInThreeDim()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-delicious",
   "metadata": {},
   "source": [
    "# Ex. 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39478716",
   "metadata": {},
   "source": [
    "Ridge regresssion:\n",
    "\n",
    "$$\n",
    "\\tilde Y = \\tilde X^T\\cdot \\theta,\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial\\theta_0} = -2\\sum\\limits_{i=1}^n\\left(y_i-\\theta_0-\\theta_1x_{i1}-\\dots-\\theta_p x_{ip}\\right)\n",
    "=-2(Y-X\\cdot\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial\\theta_j} =-2\\sum\\limits_{i=1}^nx_{ij}\\left(y_i-\\theta_0-\\theta_1x_{i1}-\\dots-\\theta_p x_{ip}\\right)+2\\alpha\\theta_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "\\left(\\frac{\\partial L}{\\partial\\theta_0}\\right)^*\\\\\n",
    "\\frac{\\partial L}{\\partial\\theta_1}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial L}{\\partial\\theta_p}\n",
    "\\end{bmatrix} & =\n",
    "-2\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\sum\\limits_{i=1}^nx_{i0}\\left(y_i-\\theta_0-\\theta_1x_{i1}-\\dots-\\theta_p x_{ip}\\right)\\\\\n",
    "\\sum\\limits_{i=1}^nx_{i1}\\left(y_i-\\theta_0-\\theta_1x_{i1}-\\dots-\\theta_p x_{ip}\\right)\\\\\n",
    "\\sum\\limits_{i=1}^nx_{i2}\\left(y_i-\\theta_0-\\theta_1x_{i1}-\\dots-\\theta_p x_{ip}\\right)\\\\\n",
    "\\vdots\\\\\n",
    "\\sum\\limits_{i=1}^nx_{ip}\\left(y_i-\\theta_0-\\theta_1x_{i1}-\\dots-\\theta_p x_{ip}\\right)\\\\\n",
    "\\end{bmatrix}+ 2\\alpha\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_p\n",
    "\\end{bmatrix}\\\\ & =\n",
    "-2\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & \\cdots & 1\\\\\n",
    "x_{11} & x_{21} & \\cdots & x_{n1}\\\\\n",
    "x_{12} & x_{22} & \\cdots & x_{n2}\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
    "x_{1p} & x_{2p} & \\cdots & x_{np}\\\\\n",
    "\\end{bmatrix}\\cdot\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\theta_0 - \\theta_1 x_{11} - \\theta_2 x_{12} - \\cdots - \\theta_p x_{1p}\\\\\n",
    "y_2 - \\theta_0 - \\theta_1 x_{21} - \\theta_2 x_{22} - \\cdots - \\theta_p x_{2p}\\\\\n",
    "\\vdots \\\\\n",
    "y_n - \\theta_0 - \\theta_1 x_{n1} - \\theta_2 x_{n2} - \\cdots - \\theta_p x_{np}\n",
    "\\end{bmatrix}+ 2\\alpha\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_p\n",
    "\\end{bmatrix}\\\\ & =\n",
    "-2\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "\\end{bmatrix}^T\\cdot\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\theta_0 - \\theta_1 x_{11} - \\theta_2 x_{12} - \\cdots - \\theta_p x_{1p}\\\\\n",
    "y_2 - \\theta_0 - \\theta_1 x_{21} - \\theta_2 x_{22} - \\cdots - \\theta_p x_{2p}\\\\\n",
    "\\vdots \\\\\n",
    "y_n - \\theta_0 - \\theta_1 x_{n1} - \\theta_2 x_{n2} - \\cdots - \\theta_p x_{np}\n",
    "\\end{bmatrix}+ 2\\alpha\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_p\n",
    "\\end{bmatrix}\\\\ & =\n",
    "-2\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "\\end{bmatrix}^T\\cdot\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}-\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 + \\theta_1 x_{11} + \\theta_2 x_{12} + \\cdots + \\theta_p x_{1p}\\\\\n",
    "\\theta_0 + \\theta_1 x_{21} + \\theta_2 x_{22} + \\cdots + \\theta_p x_{2p}\\\\\n",
    "\\vdots \\\\\n",
    "\\theta_0 + \\theta_1 x_{n1} + \\theta_2 x_{n2} + \\cdots + \\theta_p x_{np}\n",
    "\\end{bmatrix}\\right) + 2\\alpha\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_p\n",
    "\\end{bmatrix}\\\\\n",
    "& =-2\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "\\end{bmatrix}^T\\cdot\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}-\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "\\end{bmatrix}\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_p\n",
    "\\end{bmatrix}\\right) + 2\\alpha\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_p\n",
    "\\end{bmatrix}\\\\\n",
    "& =-2\\cdot X^T\\cdot(Y-X\\cdot\\theta) + 2\\alpha\\theta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122aa265",
   "metadata": {},
   "source": [
    "\\* here it's calculated the same way as for the other $j$ but then is substituted with the correct formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-explosion",
   "metadata": {},
   "source": [
    "For gradient:\n",
    "$$\n",
    "\\begin{align}\n",
    "& -2\\cdot (Y-X\\cdot\\theta)\\\\\n",
    "& -2\\cdot X^T\\cdot(Y-X\\cdot\\theta) + 2\\alpha\\theta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-insulation",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{n+1} = \\theta_{n} - c\\nabla_\\theta L\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-vietnam",
   "metadata": {},
   "source": [
    "* $c$ - learning rate\n",
    "* $L$ - cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "geographic-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "class RidgeRegr:\n",
    "    def __init__(self, alpha = 0.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # input:\n",
    "        #  X = np.array, shape = (n, m)\n",
    "        #  Y = np.array, shape = (n)\n",
    "        # Finds theta (approximately) minimising quadratic loss function L with Ridge penalty,\n",
    "        # using an iterative method.\n",
    "        n, m = X.shape\n",
    "        X = np.c_[np.ones(n), X]\n",
    "        \n",
    "        theta = np.zeros(m+1)\n",
    "        c = 5*10**(-3)\n",
    "        iter_count = 5*10**4\n",
    "        for i in range(iter_count):\n",
    "            L_grad = -2*(X.T @ (Y-X @ theta)) + 2 * self.alpha * theta\n",
    "            L_grad[0] = -2*sum(Y-X @ theta)\n",
    "            theta = theta - c * L_grad\n",
    "        self.theta = theta\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # input:\n",
    "        #  X = np.array, shape = (k, m)\n",
    "        # returns:\n",
    "        #  Y = wektor(f(X_1), ..., f(X_k))\n",
    "        k, m = X.shape\n",
    "        X = np.c_[np.ones(k), X]\n",
    "        Y = X @ self.theta\n",
    "        return Y\n",
    "\n",
    "\n",
    "def test_RidgeRegressionInOneDim():\n",
    "    X = np.array([1,3,2,5]).reshape((4,1))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    X_test = np.array([1,2,10]).reshape((3,1))\n",
    "    alpha = 0.3\n",
    "    expected = Ridge(alpha).fit(X, Y).predict(X_test)\n",
    "    actual = RidgeRegr(alpha).fit(X, Y).predict(X_test)\n",
    "    print(f'actual: {actual}')\n",
    "    print(f'expected: {expected}')\n",
    "    assert list(actual) == pytest.approx(list(expected), rel=1e-5)\n",
    "\n",
    "def test_RidgeRegressionInThreeDim():\n",
    "    X = np.array([1,2,3,5,4,5,4,3,3,3,2,5]).reshape((4,3))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    X_test = np.array([1,0,0, 0,1,0, 0,0,1, 2,5,7, -2,0,3]).reshape((5,3))\n",
    "    alpha = 0.4\n",
    "    expected = Ridge(alpha).fit(X, Y).predict(X_test)\n",
    "    actual = RidgeRegr(alpha).fit(X, Y).predict(X_test)\n",
    "    print(f'actual: {actual}')\n",
    "    print(f'expected: {expected}')\n",
    "    assert list(actual) == pytest.approx(list(expected), rel=1e-3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lasting-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: [ 1.88950276  3.38121547 15.31491713]\n",
      "expected: [ 1.88950276  3.38121547 15.31491713]\n"
     ]
    }
   ],
   "source": [
    "test_RidgeRegressionInOneDim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "twelve-restoration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: [ 0.54685378 -1.76188321  1.58691716  5.15527388  3.66704391]\n",
      "expected: [ 0.54685378 -1.76188321  1.58691716  5.15527388  3.66704391]\n"
     ]
    }
   ],
   "source": [
    "test_RidgeRegressionInThreeDim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-enzyme",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
